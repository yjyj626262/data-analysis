# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import matplotlib.pyplot as plt
import seaborn as sns

import random

import dask
import dask.array as da
import dask.dataframe as dd
%%time
import polars as pl
train_series = (pl.scan_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/train_series.parquet')
                .with_columns(
                    (
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.year().alias("year")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.month().alias("month")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.day().alias("day")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.hour().alias("hour")),
                    )
                )
                .collect()
                .to_pandas()
               )
CPU times: user 1min 24s, sys: 30.5 s, total: 1min 55s
Wall time: 1min 17s
Memory reduction
The following function has been taken from the notebook ðŸ› Feat Eng ideas & 60% memory reduction ðŸ› - CMI

from pandas.api.types import is_datetime64_ns_dtype
import gc

import warnings
warnings.filterwarnings("ignore")

def reduce_mem_usage(df):
    
    """ 
    Iterate through all numeric columns of a dataframe and modify the data type
    to reduce memory usage.        
    """
    
    start_mem = df.memory_usage().sum() / 1024**2
    print(f'Memory usage of dataframe is {start_mem:.2f} MB')
    
    for col in df.columns:
        col_type = df[col].dtype

        if col_type != object and not is_datetime64_ns_dtype(df[col]):
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float32)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        

    df['series_id'] = df['series_id'].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print(f'Memory usage after optimization is: {end_mem:.2f} MB')
    decrease = 100 * (start_mem - end_mem) / start_mem
    print(f'Decreased by {decrease:.2f}%')
    
    return df
train_series = reduce_mem_usage(train_series)
Memory usage of dataframe is 3416.54 MB
Memory usage after optimization is: 2684.43 MB
Decreased by 21.43%
train_series.head()
series_id	step	timestamp	anglez	enmo
0	038441c925bb	0.0	2018-08-14 15:30:00	2.6367	0.0217
1	038441c925bb	1.0	2018-08-14 15:30:05	2.6368	0.0215
2	038441c925bb	2.0	2018-08-14 15:30:10	2.6370	0.0216
3	038441c925bb	3.0	2018-08-14 15:30:15	2.6368	0.0213
4	038441c925bb	4.0	2018-08-14 15:30:20	2.6368	0.0215
train_events = (pl.scan_csv('/kaggle/input/child-mind-institute-detect-sleep-states/train_events.csv')
                .with_columns(
                    (
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z")),
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.year().alias("year")),
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.month().alias("month")),
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.day().alias("day")),
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.hour().alias("hour")),
                    )
                )
                .collect()
                .to_pandas()
               )
train_events.head()
series_id	night	event	step	timestamp	year	month	day	hour
0	038441c925bb	1	onset	4992.0	2018-08-14 22:26:00	2018.0	8.0	14.0	22.0
1	038441c925bb	1	wakeup	10932.0	2018-08-15 06:41:00	2018.0	8.0	15.0	6.0
2	038441c925bb	2	onset	20244.0	2018-08-15 19:37:00	2018.0	8.0	15.0	19.0
3	038441c925bb	2	wakeup	27492.0	2018-08-16 05:41:00	2018.0	8.0	16.0	5.0
4	038441c925bb	3	onset	39996.0	2018-08-16 23:03:00	2018.0	8.0	16.0	23.0
Preprocess the data
Sort the datasets by the timestamp
train_series = train_series.sort_values('timestamp').reset_index(drop=True)
train_series.head()
series_id	step	timestamp	anglez	enmo
0	fe90110788d2	0.0	2017-08-04 17:30:00	-27.707001	0.0298
1	fe90110788d2	1.0	2017-08-04 17:30:05	-33.867500	0.0488
2	fe90110788d2	2.0	2017-08-04 17:30:10	-15.475000	0.1077
3	fe90110788d2	3.0	2017-08-04 17:30:15	-73.656197	0.0530
4	fe90110788d2	4.0	2017-08-04 17:30:20	-53.152901	0.0601
train_events = train_events.sort_values('timestamp').reset_index(drop=True)
train_events.head()
series_id	night	event	step	timestamp	year	month	day	hour
0	fe90110788d2	2	onset	21048.0	2017-08-05 22:44:00	2017.0	8.0	5.0	22.0
1	fe90110788d2	2	wakeup	27852.0	2017-08-06 08:11:00	2017.0	8.0	6.0	8.0
2	fe90110788d2	3	onset	38064.0	2017-08-06 22:22:00	2017.0	8.0	6.0	22.0
3	fe90110788d2	3	wakeup	42384.0	2017-08-07 04:22:00	2017.0	8.0	7.0	4.0
4	fe90110788d2	4	onset	54060.0	2017-08-07 20:35:00	2017.0	8.0	7.0	20.0
Sleep hours distribution
Create the dataframe with the sleep hours as follow:

Make the dfference between the rows of the timestamp columns
Extract the hour and day components from the difference
Encode the events and make the difference
Create a pandas dataframe named train_sleep
Since there are missing values in the dataframe that can make the difference big, it is good to remove the rows with day > 0 (sssuming they have never slept for more than 24 hours)
After the difference, the events are equal to -1 and +1, corresponding to hours of sleep and hours of wakefulness, respectively.
For our purpose, only the events = -1 will be kept.

### Difference between the rows of the timestamp column
train_events_hours = train_events.timestamp.diff().dt.components['hours']   # Extract the hour component from the difference
train_events_days  = train_events.timestamp.diff().dt.components['days']    # Extract the day component from the difference
train_events_event = train_events.event.map({'onset':1, 'wakeup':0}).diff() # Encode the events and make the difference
### Create the pandas dataframe
train_sleep = pd.DataFrame([train_events.month, train_events.day, train_events_days, train_events_hours, train_events_event], 
                           index=['month', 'day', 'days_sleep', 'hours_sleep', 'event']).T.dropna()
train_sleep = train_sleep[(train_sleep.days_sleep == 0) & (train_sleep.event == -1)].reset_index(drop=True)
train_sleep.head()
month	day	days_sleep	hours_sleep	event
0	8.0	6.0	0.0	9.0	-1.0
1	8.0	7.0	0.0	6.0	-1.0
2	8.0	8.0	0.0	12.0	-1.0
3	8.0	9.0	0.0	6.0	-1.0
4	8.0	10.0	0.0	5.0	-1.0
train_sleep.describe()
month	day	days_sleep	hours_sleep	event
count	735.000000	735.000000	735.0	735.000000	735.0
mean	6.512925	15.834014	0.0	3.994558	-1.0
std	3.519821	8.865307	0.0	2.200439	0.0
min	1.000000	1.000000	0.0	0.000000	-1.0
25%	3.000000	8.000000	0.0	2.000000	-1.0
50%	6.000000	16.000000	0.0	4.000000	-1.0
75%	10.000000	24.000000	0.0	6.000000	-1.0
max	12.000000	31.000000	0.0	12.000000	-1.0
Interpolation of the timestamp
train_events.head(15)
series_id	night	event	step	timestamp	year	month	day	hour
0	fe90110788d2	2	onset	21048.0	2017-08-05 22:44:00	2017.0	8.0	5.0	22.0
1	fe90110788d2	2	wakeup	27852.0	2017-08-06 08:11:00	2017.0	8.0	6.0	8.0
2	fe90110788d2	3	onset	38064.0	2017-08-06 22:22:00	2017.0	8.0	6.0	22.0
3	fe90110788d2	3	wakeup	42384.0	2017-08-07 04:22:00	2017.0	8.0	7.0	4.0
4	fe90110788d2	4	onset	54060.0	2017-08-07 20:35:00	2017.0	8.0	7.0	20.0
5	fe90110788d2	4	wakeup	62760.0	2017-08-08 08:40:00	2017.0	8.0	8.0	8.0
6	fe90110788d2	5	onset	72804.0	2017-08-08 22:37:00	2017.0	8.0	8.0	22.0
7	ece2561f07e9	1	onset	8976.0	2017-08-09 00:43:00	2017.0	8.0	9.0	0.0
8	fe90110788d2	5	wakeup	78876.0	2017-08-09 07:03:00	2017.0	8.0	9.0	7.0
9	ece2561f07e9	1	wakeup	15972.0	2017-08-09 10:26:00	2017.0	8.0	9.0	10.0
10	fe90110788d2	6	onset	88824.0	2017-08-09 20:52:00	2017.0	8.0	9.0	20.0
11	ece2561f07e9	2	onset	26208.0	2017-08-10 00:39:00	2017.0	8.0	10.0	0.0
12	fe90110788d2	6	wakeup	95292.0	2017-08-10 05:51:00	2017.0	8.0	10.0	5.0
13	ece2561f07e9	2	wakeup	32568.0	2017-08-10 09:29:00	2017.0	8.0	10.0	9.0
14	fe90110788d2	7	onset	108072.0	2017-08-10 23:36:00	2017.0	8.0	10.0	23.0
train_events['timestamp'] = train_events.timestamp.interpolate()
train_events.head(15)
series_id	night	event	step	timestamp	year	month	day	hour
0	fe90110788d2	2	onset	21048.0	2017-08-05 22:44:00	2017.0	8.0	5.0	22.0
1	fe90110788d2	2	wakeup	27852.0	2017-08-06 08:11:00	2017.0	8.0	6.0	8.0
2	fe90110788d2	3	onset	38064.0	2017-08-06 22:22:00	2017.0	8.0	6.0	22.0
3	fe90110788d2	3	wakeup	42384.0	2017-08-07 04:22:00	2017.0	8.0	7.0	4.0
4	fe90110788d2	4	onset	54060.0	2017-08-07 20:35:00	2017.0	8.0	7.0	20.0
5	fe90110788d2	4	wakeup	62760.0	2017-08-08 08:40:00	2017.0	8.0	8.0	8.0
6	fe90110788d2	5	onset	72804.0	2017-08-08 22:37:00	2017.0	8.0	8.0	22.0
7	ece2561f07e9	1	onset	8976.0	2017-08-09 00:43:00	2017.0	8.0	9.0	0.0
8	fe90110788d2	5	wakeup	78876.0	2017-08-09 07:03:00	2017.0	8.0	9.0	7.0
9	ece2561f07e9	1	wakeup	15972.0	2017-08-09 10:26:00	2017.0	8.0	9.0	10.0
10	fe90110788d2	6	onset	88824.0	2017-08-09 20:52:00	2017.0	8.0	9.0	20.0
11	ece2561f07e9	2	onset	26208.0	2017-08-10 00:39:00	2017.0	8.0	10.0	0.0
12	fe90110788d2	6	wakeup	95292.0	2017-08-10 05:51:00	2017.0	8.0	10.0	5.0
13	ece2561f07e9	2	wakeup	32568.0	2017-08-10 09:29:00	2017.0	8.0	10.0	9.0
14	fe90110788d2	7	onset	108072.0	2017-08-10 23:36:00	2017.0	8.0	10.0	23.0
Merge the event and series datasets
train_series.shape
(127946340, 5)
train_events_tomerge = train_events[['timestamp', 'event']]
train_events_tomerge.head()
timestamp	event
0	2017-08-05 22:44:00	onset
1	2017-08-06 08:11:00	wakeup
2	2017-08-06 22:22:00	onset
3	2017-08-07 04:22:00	wakeup
4	2017-08-07 20:35:00	onset
train_events_tomerge.isna().sum()
timestamp    0
event        0
dtype: int64
train_series_merged = train_series.merge(train_events_tomerge, how='left', on='timestamp')
train_series_merged.head()
series_id	step	timestamp	anglez	enmo	event
0	fe90110788d2	0.0	2017-08-04 17:30:00	-27.707001	0.0298	NaN
1	fe90110788d2	1.0	2017-08-04 17:30:05	-33.867500	0.0488	NaN
2	fe90110788d2	2.0	2017-08-04 17:30:10	-15.475000	0.1077	NaN
3	fe90110788d2	3.0	2017-08-04 17:30:15	-73.656197	0.0530	NaN
4	fe90110788d2	4.0	2017-08-04 17:30:20	-53.152901	0.0601	NaN
Fill the missing values in the event column
Using the method 'ffill', the missing values will be replace with the previous non-missing value

train_series_merged['event'] = train_series_merged.event.fillna(method='ffill')
Acting like this,the first rows with missing values are not replaced because there is no value before them.
Checking the train_events dataset, the first event is 'onset', that correspond to the beginning of sleep.
For this reason the event before 'onset' is 'wakeup'.
Let's fill the remaining rows with missing values with 'wakeup'.

train_series_merged['event'] = train_series_merged.event.fillna('wakeup')
Check for missing values in the event column

train_series_merged.event.isna().sum()
0
Explore the data
Data information
train_events.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 14508 entries, 0 to 14507
Data columns (total 9 columns):
 #   Column     Non-Null Count  Dtype         
---  ------     --------------  -----         
 0   series_id  14508 non-null  object        
 1   night      14508 non-null  int64         
 2   event      14508 non-null  object        
 3   step       9585 non-null   float64       
 4   timestamp  14508 non-null  datetime64[ns]
 5   year       9585 non-null   float64       
 6   month      9585 non-null   float64       
 7   day        9585 non-null   float64       
 8   hour       9585 non-null   float64       
dtypes: datetime64[ns](1), float64(5), int64(1), object(2)
memory usage: 1020.2+ KB
Explore the train_events dataset
train_events.describe()
night	step	timestamp	year	month	day	hour
count	14508.000000	9585.000000	14508	9585.000000	9585.000000	9585.000000	9585.000000
mean	15.120072	214352.123944	2018-11-03 10:38:26.277915904	2017.997809	6.496609	15.987585	12.127804
min	1.000000	936.000000	2017-08-05 22:44:00	2017.000000	1.000000	1.000000	0.000000
25%	7.000000	95436.000000	2018-03-31 05:47:15	2018.000000	3.000000	8.000000	6.000000
50%	14.000000	200604.000000	2018-12-19 22:44:30	2018.000000	6.000000	16.000000	8.000000
75%	21.000000	317520.000000	2019-07-05 05:33:00	2018.000000	10.000000	24.000000	21.000000
max	84.000000	739392.000000	2019-07-05 05:33:00	2019.000000	12.000000	31.000000	23.000000
std	10.286758	141268.408192	NaN	0.660645	3.667130	8.818433	7.944063
print('Number of unique identifiers for each series of accelerometer data:', len(train_events.series_id.unique()))
Number of unique identifiers for each series of accelerometer data: 277
Hours distribution
plt.figure(figsize=(10,4))
# plt.subplot(121)
plt.title('Hour distribution with onset and wakeup')
sns.histplot(x=train_events.dropna().hour, hue=train_events.dropna().event, stat='density', bins=24, binrange=(-0.5, 23.5))
sns.kdeplot(train_events.dropna().hour, bw_adjust=0.45)
<Axes: title={'center': 'Hour distribution with onset and wakeup'}, xlabel='hour', ylabel='Density'>

plt.figure(figsize=(10,4))
plt.title('Sleep hours distribution')
sns.histplot(x=train_sleep.hours_sleep, stat='density', bins=19, binrange=(-0.5, 18.5))
sns.kdeplot(train_sleep.hours_sleep, bw_adjust=2, color='red')
<Axes: title={'center': 'Sleep hours distribution'}, xlabel='hours_sleep', ylabel='Density'>

plt.figure(figsize=(15,3))
plt.title('Average sleep hours per day')
sns.barplot(x=train_sleep.groupby('day').hours_sleep.mean().index, y=train_sleep.groupby('day').hours_sleep.mean().values)
<Axes: title={'center': 'Average sleep hours per day'}, xlabel='day'>

plt.figure(figsize=(10,3))
plt.title('Average sleep hours per month')
sns.barplot(x=train_sleep.groupby('month').hours_sleep.mean().index, y=train_sleep.groupby('month').hours_sleep.mean().values)
<Axes: title={'center': 'Average sleep hours per month'}, xlabel='month'>

The average number of sleep hour is about 8 hours. There is no distinction between days and months. It could have been hypothesized that in the summer months and on weekends, the hours of sleep were greater. This is because, considering that in these days the parents, not working, would have been able to sleep more, the children's sleeping hours would have increased. This hypothesis is not visible in the data and is therefore rejected.

Explore the train_series_merged dataset
train_series.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 127946340 entries, 0 to 127946339
Data columns (total 5 columns):
 #   Column     Dtype         
---  ------     -----         
 0   series_id  category      
 1   step       float32       
 2   timestamp  datetime64[ns]
 3   anglez     float32       
 4   enmo       float32       
dtypes: category(1), datetime64[ns](1), float32(3)
memory usage: 2.6 GB
train_series.isna().sum()
series_id    0
step         0
timestamp    0
anglez       0
enmo         0
dtype: int64
print(f'In the dataset there are {len(train_series.series_id.unique())} accelerometer series')
In the dataset there are 277 accelerometer series
plt.figure(figsize=(14,3))
sns.barplot(x=train_series.groupby('series_id').anglez.mean(),
            y=train_series.groupby('series_id').anglez.mean())
g = plt.xticks(ticks=[])

Take 5 random series_id and explore them

series_id_selected = random.choices(train_series_merged.series_id.unique(), k=5)
series_id_selected
['f6d2cc003183',
 '0402a003dae9',
 'eef041dd50aa',
 '7476c0bd18d2',
 '27f09a6a858f']
Extract a subset of the dataset with the selected series id

train_series_sub = train_series_merged[train_series_merged.series_id.isin(series_id_selected)].reset_index(drop=True)
train_series_sub.shape
(2028999, 6)
Convert from datetime to timestamp
train_series_sub['timestamp_new'] = train_series_sub.timestamp.astype(int).div(10**9)
train_series_sub.head()
series_id	step	timestamp	anglez	enmo	event	timestamp_new
0	f6d2cc003183	0.0	2017-08-14 12:30:00	-22.963499	0.1006	wakeup	1.502714e+09
1	f6d2cc003183	1.0	2017-08-14 12:30:05	-22.918501	0.1967	wakeup	1.502714e+09
2	f6d2cc003183	2.0	2017-08-14 12:30:10	-67.985603	0.1601	wakeup	1.502714e+09
3	f6d2cc003183	3.0	2017-08-14 12:30:15	-57.648602	0.2976	wakeup	1.502714e+09
4	f6d2cc003183	4.0	2017-08-14 12:30:20	-50.711498	0.0946	wakeup	1.502714e+09
%%time
plt.figure(figsize=(12,3))
sns.scatterplot(data=train_series_sub[train_series_sub.series_id == series_id_selected[0]],
             x='timestamp', y='anglez', hue='event')
CPU times: user 869 ms, sys: 69 ms, total: 938 ms
Wall time: 899 ms
<Axes: xlabel='timestamp', ylabel='anglez'>

%%time
plt.figure(figsize=(12,3))
sns.scatterplot(x=train_series_sub[train_series_sub.series_id == series_id_selected[0]].timestamp,
                y=train_series_sub[train_series_sub.series_id == series_id_selected[0]].anglez*train_series_sub[train_series_sub.series_id == series_id_selected[0]].enmo,
                hue=train_series_sub[train_series_sub.series_id == series_id_selected[0]].event)
CPU times: user 909 ms, sys: 69.8 ms, total: 979 ms
Wall time: 950 ms
<Axes: xlabel='timestamp'>

%%time
for ids in series_id_selected:
    plt.figure(figsize=(12,3))
    plt.title(f'z-angle distribution with events for {ids} id')
    sns.histplot(data=train_series_sub[train_series_sub.series_id == ids],
                 x=train_series_sub[train_series_sub.series_id == ids].anglez.abs(), hue='event', fill=True, bins=100, alpha=0.3)
    plt.show()





CPU times: user 6.77 s, sys: 833 ms, total: 7.6 s
Wall time: 6.38 s
%%time
for ids in series_id_selected:
    plt.figure(figsize=(12,3))
    plt.title(f'z-angle distribution with events for {ids} id')
    sns.histplot(data=train_series_sub[train_series_sub.series_id == ids],
                 x='enmo', hue='event', fill=True, bins=100, alpha=0.3)
    plt.yscale('log')
    plt.show()





CPU times: user 9.24 s, sys: 791 ms, total: 10 s
Wall time: 8.83 s
From the z-angle distribution follows that:

the wakeup probability decrases as the absolute value of the angle increases.
the onset probability is almost constant for all the angle values, except for a few peaks
From the ENMO distribution both the event count decreases as the enmo encreases

%%time
plt.figure(figsize=(12,3))
plt.title('z-angle distribution with events')
sns.histplot(data=train_series_sub[train_series_sub.series_id == series_id_selected[0]],
             x=np.sin(train_series_sub[train_series_sub.series_id == series_id_selected[0]].anglez)*\
             (train_series_sub[train_series_sub.series_id == series_id_selected[0]].enmo), 
             hue='event', fill=True, bins=100, alpha=0.3)
plt.yscale('log')
CPU times: user 890 ms, sys: 64.9 ms, total: 955 ms
Wall time: 833 ms

In the ENMO, all the zero values correspond to zero or negative values rounded to zero.

train_series_merged.memory_usage().sum() / 1024**2
3660.9473037719727
Preprocess the test dataset
%%time
test_series = (pl.scan_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/test_series.parquet')
                .with_columns(
                    (
                        (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.year().alias("year")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.month().alias("month")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.day().alias("day")),
#                         (pl.col("timestamp").str.strptime(pl.Datetime, "%Y-%m-%dT%H:%M:%S%Z").dt.hour().alias("hour")),
                    )
                )
                .collect()
                .to_pandas()
               )
CPU times: user 4.98 ms, sys: 1.6 ms, total: 6.58 ms
Wall time: 11.8 ms
test_series = reduce_mem_usage(test_series)
Memory usage of dataframe is 0.01 MB
Memory usage after optimization is: 0.01 MB
Decreased by 23.71%
Save the datasets
train_series_merged.to_parquet('/kaggle/working/train_series_preprocessed.parquet')
test_series.to_parquet('/kaggle/working/test_series_preprocessed.parquet')
Since the dataset is large, to avoid long times, the model will be made in the notebook Detect Sleep States: Model
The saved dataset is Sleep States Preprocessed Dataset
